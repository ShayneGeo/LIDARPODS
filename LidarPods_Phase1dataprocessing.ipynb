{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680339e3-eaef-4fed-b6e6-97efad0627ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Lidar Data Retrieval (tiles) and Processing for Wildfire Perimeters\n",
    "\n",
    "# This section of code automates the retrieval and processing of lidar data for wildfire perimeters.\n",
    "# It iterates over a list of wildfire perimeter shapefiles, retrieves lidar data from the Planetary Computer STAC API,\n",
    "# and saves the downloaded GeoTIFF files as tiles to a newly created folder, named after each fire perimeter, within an output directory.\n",
    "\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pystac_client\n",
    "from shapely.geometry import shape, Polygon, Point\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import planetary_computer\n",
    "import random\n",
    "\n",
    "# Input to individual fire perimeters\n",
    "InputFolder = \"D:\\\\LIDARPODSPHASE1\\\\InputData\\\\IndividualPerimeters\"\n",
    "\n",
    "# Output tile locations\n",
    "outputfolder = \"D:\\\\LIDARPODSPHASE1\\\\IntermediateData\\\\\"\n",
    "\n",
    "# List of perimeter shapefiles\n",
    "perimeterlist = [\n",
    "    os.path.join(InputFolder, \"AugustComplexFire_2020_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"Dixie_2021_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"BearNorthComplex_2020_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"Bush_2020_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"CalfCanyon_2022_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"CameronPeak_2020_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"Rafael_2021_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"Salt1_2020_perimeter.shp\"),\n",
    "    os.path.join(InputFolder, \"Sugar_2021_perimeter.shp\"),\n",
    "    os.path.join(outputfolder, \"Telegraph_2021_perimeter.shp\")\n",
    "]\n",
    "\n",
    "# Year of fire for end search data for lidar\n",
    "FireYear = [\n",
    "    2020,\n",
    "    2021,\n",
    "    2020,\n",
    "    2020,\n",
    "    2022,\n",
    "    2020,\n",
    "    2021,\n",
    "    2020, \n",
    "    2021,\n",
    "    2021\n",
    "]\n",
    "\n",
    "# Define the collections for LIDAR - Height Above Ground and Digital Terrain Model\n",
    "COLLECTIONLIST = [\"3dep-lidar-hag\", \"3dep-lidar-dtm\"]\n",
    "\n",
    "# Accesses the Planetary Computer's STAC API using the pystac_client module\n",
    "catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=planetary_computer.sign_inplace)\n",
    "\n",
    "for fire_perimeter_shapefile, fire_year in zip(perimeterlist, FireYear):\n",
    "    # Define the start and end dates for the search\n",
    "    start_date = datetime(2013, 1, 1)\n",
    "    end_date = datetime(fire_year - 1, 12, 31) \n",
    "    \n",
    "    # Read the fire perimeter shapefile\n",
    "    fire_perimeter_gdf = gpd.read_file(fire_perimeter_shapefile)\n",
    "\n",
    "    # Create a folder for the current fire perimeter\n",
    "    fire_perimeter_folder = os.path.join(outputfolder, os.path.splitext(os.path.basename(fire_perimeter_shapefile))[0])\n",
    "    os.makedirs(fire_perimeter_folder, exist_ok=True)\n",
    "\n",
    "    # Calculate the bounding box of the fire perimeter\n",
    "    xmin, ymin, xmax, ymax = fire_perimeter_gdf.total_bounds\n",
    "    bbox_polygon_geojson = {\"type\": \"Polygon\", \"coordinates\": [[\n",
    "        [xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin], [xmin, ymin]\n",
    "    ]]}\n",
    "\n",
    "    # Convert the GeoJSON Polygon to a Shapely Polygon\n",
    "    bbox_polygon = shape(bbox_polygon_geojson)\n",
    "    buffer_distance = 0.4  # Buffer distance in degrees\n",
    "    buffered_polygon = bbox_polygon.buffer(buffer_distance)\n",
    "    buffered_polygon_geojson = buffered_polygon.__geo_interface__\n",
    "\n",
    "    # Iterate over each collection\n",
    "    for collection in COLLECTIONLIST:\n",
    "        srch_hag = catalog.search(collections=collection, intersects=buffered_polygon_geojson,\n",
    "                                  datetime=f\"{start_date.isoformat()}/{end_date.isoformat()}\")\n",
    "        ic_hag = srch_hag.item_collection()\n",
    "        \n",
    "        if len(ic_hag) == 0:\n",
    "            print(\"The ItemCollection is empty.\")\n",
    "        else:\n",
    "            print(f\"The ItemCollection contains {len(ic_hag)} items.\")\n",
    "        \n",
    "        for i, item in enumerate(ic_hag):\n",
    "            asset_url = item.assets['data'].href\n",
    "            output_tif_path = os.path.join(fire_perimeter_folder, f\"{os.path.splitext(os.path.basename(fire_perimeter_shapefile))[0]}_{collection}_{i}.tiff\")\n",
    "            response = requests.get(asset_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(output_tif_path, 'wb') as output_file:\n",
    "                    output_file.write(response.content)\n",
    "                print(f\"GeoTIFF file '{output_tif_path}' has been downloaded.\")\n",
    "            else:\n",
    "                print(f\"Failed to download the asset.\")\n",
    "            time.sleep(3)\n",
    "\n",
    "\n",
    "# Define the pattern to make it more interesting with randomness\n",
    "def print_random_pattern(rows, cols):\n",
    "    symbols = [\"D\", \"O\", \"N\", \"E\", \"!\"]  # List of symbols to choose from\n",
    "    for _ in range(rows):\n",
    "        pattern = ''.join(random.choice(symbols) for _ in range(cols))  # Generate a random pattern for each row\n",
    "        print(pattern)\n",
    "\n",
    "# Customize the parameters here\n",
    "rows = 20  # Number of lines\n",
    "cols = 100  # Number of symbols in each line\n",
    "\n",
    "# Call the function with the customized parameters\n",
    "print_random_pattern(rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05497dd-a371-41a3-b8e8-52124c70bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging LiDAR Data \n",
    "\n",
    "# This script automates the processing of LiDAR data \n",
    "# stored in multiple directories within a specified output folder. \n",
    "# It first merges data (HAG and DTM) for each fire perimeter then saves \n",
    "# merged files to an output directory (OutputMerge)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "# Output folder where merged files will be saved\n",
    "OutputMerge = \"D:\\\\LIDARPODSPHASE1\\\\OutputData\\\\\"\n",
    "\n",
    "# Find all subdirectories in the output folder\n",
    "fire_perimeter_folders = [f.path for f in os.scandir(outputfolder) if f.is_dir()]\n",
    "\n",
    "for fire_perimeter_folder in fire_perimeter_folders:\n",
    "    # Extract the fire perimeter name from the folder path\n",
    "    fire_perimeter_name = os.path.basename(fire_perimeter_folder)\n",
    "    \n",
    "    # Find all TIFF files that contain 'hag' in the filename within the fire perimeter folder\n",
    "    hag_tifs = glob.glob(os.path.join(fire_perimeter_folder, \"*3dep-lidar-hag*.tiff\"))\n",
    "\n",
    "    # Prepare the output file path for HAG\n",
    "    output_tif_hag = os.path.join(OutputMerge, f\"merged_{fire_perimeter_name}_hag.tif\")\n",
    "\n",
    "    # Prepare the gdal_merge command for HAG\n",
    "    merge_command_hag = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\magst\\\\anaconda3\\\\envs\\\\LIDAR\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif_hag,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + hag_tifs\n",
    "\n",
    "    # Run the gdal_merge command for HAG and capture the output\n",
    "    process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for HAG was successful\n",
    "    if process_hag.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files HAG for {fire_perimeter_name}:\")\n",
    "        print(process_hag.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for HAG for {fire_perimeter_name}.\")\n",
    "\n",
    "    # Find all TIFF files that contain 'dtm' in the filename within the fire perimeter folder\n",
    "    dtm_tifs = glob.glob(os.path.join(fire_perimeter_folder, \"*3dep-lidar-dtm*.tiff\"))\n",
    "\n",
    "    # Prepare the output file path for DTM\n",
    "    output_tif_dtm = os.path.join(OutputMerge, f\"merged_{fire_perimeter_name}_dtm.tif\")\n",
    "\n",
    "    # Prepare the gdal_merge command for DTM\n",
    "    merge_command_dtm = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\magst\\\\anaconda3\\\\envs\\\\LIDAR\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif_dtm,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + dtm_tifs\n",
    "\n",
    "    # Run the gdal_merge command for DTM and capture the output\n",
    "    process_dtm = subprocess.run(merge_command_dtm, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for DTM was successful\n",
    "    if process_dtm.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files DTM for {fire_perimeter_name}:\")\n",
    "        print(process_dtm.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for DTM for {fire_perimeter_name}.\")\n",
    "\n",
    "print(\"All MERGING processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85356c6f-5faf-4ae8-9a30-bc6b69a1e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOPE LOOP ALL FIRES\n",
    "\n",
    "# This code ingests DTM merged files for each fire perimeter and \n",
    "# generates a slope layer and exports to the merged file location\n",
    "\n",
    "from osgeo import gdal, osr\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import os \n",
    "\n",
    "# Output Merged DTM location\n",
    "#OutputMerge = \"D:\\\\LIDARPODSPHASE1\\\\OutputData\\\\\"\n",
    "\n",
    "# Find all DTM files in the directory\n",
    "dtm_files = glob.glob(os.path.join(OutputMerge, \"*dtm.tif\"))\n",
    "\n",
    "def calculate_slope(elevation, no_data_value):\n",
    "    # Replace no-data values with NaN for slope calculation\n",
    "    elevation[elevation == no_data_value] = np.nan\n",
    "\n",
    "    # Calculate the slope\n",
    "    x, y = np.gradient(elevation, edge_order=2)\n",
    "    slope = np.sqrt(x**2 + y**2)\n",
    "    slope = np.arctan(slope) * 180.0 / np.pi  # Convert to degrees\n",
    "\n",
    "    # Replace NaN back with no-data value\n",
    "    slope[np.isnan(slope)] = no_data_value\n",
    "    return slope\n",
    "\n",
    "for dtm_file in dtm_files:\n",
    "    # Open the DTM raster file\n",
    "    dtm = gdal.Open(dtm_file)\n",
    "    band = dtm.GetRasterBand(1)\n",
    "\n",
    "    # Get the size and georeferencing of the input raster\n",
    "    xsize, ysize = band.XSize, band.YSize\n",
    "    geotransform = dtm.GetGeoTransform()\n",
    "    projection = dtm.GetProjection()\n",
    "\n",
    "    # Define the data type and no data value for the output raster\n",
    "    data_type = gdal.GDT_Float32\n",
    "    no_data_value = -9999\n",
    "\n",
    "    # Define the output slope file name\n",
    "    output_slope_file = dtm_file.replace(\"_dtm.tif\", \"_slope.tif\")\n",
    "\n",
    "    # Create an output raster file\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_raster = driver.Create(output_slope_file, xsize, ysize, 1, data_type)\n",
    "    out_raster.SetGeoTransform(geotransform)\n",
    "    out_raster.SetProjection(projection)\n",
    "    out_band = out_raster.GetRasterBand(1)\n",
    "    out_band.SetNoDataValue(no_data_value)\n",
    "    out_band.FlushCache()\n",
    "\n",
    "    # Define the size of the chunks\n",
    "    chunk_size = 1000  # Adjust based on your system's memory\n",
    "\n",
    "    total_chunks = ((xsize - 1) // chunk_size + 1) * ((ysize - 1) // chunk_size + 1)\n",
    "    processed_chunks = 0\n",
    "    next_progress_threshold = 10\n",
    "\n",
    "    # Process the raster in chunks and write to output file\n",
    "    for i in range(0, xsize, chunk_size):\n",
    "        for j in range(0, ysize, chunk_size):\n",
    "            xoff = i\n",
    "            yoff = j\n",
    "            win_xsize = min(chunk_size, xsize - i)\n",
    "            win_ysize = min(chunk_size, ysize - j)\n",
    "    \n",
    "            elevation = band.ReadAsArray(xoff, yoff, win_xsize, win_ysize)\n",
    "            slope_chunk = calculate_slope(elevation, no_data_value)\n",
    "            \n",
    "            processed_chunks += 1\n",
    "            progress = (processed_chunks / total_chunks) * 100\n",
    "            \n",
    "            if progress >= next_progress_threshold:\n",
    "                print(f\"Processing... {progress:.2f}% complete\")\n",
    "                next_progress_threshold += 10\n",
    "    \n",
    "            # Write the slope_chunk to the output raster\n",
    "            out_band.WriteArray(slope_chunk, xoff, yoff)\n",
    "    \n",
    "            gc.collect()\n",
    "    # Close the output raster\n",
    "    out_band.FlushCache()\n",
    "    out_raster = None\n",
    "\n",
    "    print(f\"Slope raster saved to {output_slope_file}\")\n",
    "\n",
    "    # Collect garbage after processing each file\n",
    "    gc.collect()\n",
    "\n",
    "print(\"All SLOPE rasters processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a97e8b-c713-4dfa-bfdc-c52b8c277a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRI LOOP ALL FIRES\n",
    "\n",
    "# This code processes Digital Terrain Model (DTM) merged files for each fire perimeter \n",
    "# and generates a Terrain Ruggedness Index (TRI) layer.\n",
    "\n",
    "from osgeo import gdal, osr\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import os \n",
    "\n",
    "# Output Merged DTM location\n",
    "#OutputMerge = \"D:\\\\LIDARPODSPHASE1\\\\OutputData\\\\\"\n",
    "\n",
    "# Find all DTM files in the directory\n",
    "dtm_files = glob.glob(os.path.join(OutputMerge, \"*dtm.tif\"))\n",
    "\n",
    "def calculate_TRI(elevation, no_data_value):\n",
    "    \n",
    "    elevation[elevation == no_data_value] = np.nan\n",
    "\n",
    "    diffs = []\n",
    "    for dy in range(-1, 2):\n",
    "        for dx in range(-1, 2):\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate differences with proper edge handling\n",
    "            diff = np.empty_like(elevation)\n",
    "            diff.fill(np.nan)  # Fill with NaNs\n",
    "\n",
    "            # Define the slice ranges to avoid edge wrapping\n",
    "            sy = slice(max(0, -dy), min(elevation.shape[0], elevation.shape[0] - dy))\n",
    "            sx = slice(max(0, -dx), min(elevation.shape[1], elevation.shape[1] - dx))\n",
    "            tsy = slice(max(0, dy), min(elevation.shape[0], elevation.shape[0] + dy))\n",
    "            tsx = slice(max(0, dx), min(elevation.shape[1], elevation.shape[1] + dx))\n",
    "\n",
    "            # Compute the difference\n",
    "            diff[tsy, tsx] = elevation[tsy, tsx] - elevation[sy, sx]\n",
    "            diffs.append(diff)\n",
    "\n",
    "    # Calculate the root mean square of the differences\n",
    "    diffs = np.array(diffs)\n",
    "    rms_diff = np.sqrt(np.nanmean(diffs ** 2, axis=0))\n",
    "\n",
    "    rms_diff[np.isnan(rms_diff)] = no_data_value\n",
    "    return rms_diff\n",
    "\n",
    "for dtm_file in dtm_files:\n",
    "    # Open the DTM raster file\n",
    "    dtm = gdal.Open(dtm_file)\n",
    "    band = dtm.GetRasterBand(1)\n",
    "\n",
    "    # Get the size and georeferencing of the input raster\n",
    "    xsize, ysize = band.XSize, band.YSize\n",
    "    geotransform = dtm.GetGeoTransform()\n",
    "    projection = dtm.GetProjection()\n",
    "\n",
    "    # Define the data type and no data value for the output raster\n",
    "    data_type = gdal.GDT_Float32\n",
    "    no_data_value = -9999\n",
    "\n",
    "    # Define the output TRI file name\n",
    "    output_tri_file = dtm_file.replace(\"_dtm.tif\", \"_tri.tif\")\n",
    "\n",
    "    # Create an output raster file\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_raster = driver.Create(output_tri_file, xsize, ysize, 1, data_type)\n",
    "    out_raster.SetGeoTransform(geotransform)\n",
    "    out_raster.SetProjection(projection)\n",
    "    out_band = out_raster.GetRasterBand(1)\n",
    "    out_band.SetNoDataValue(no_data_value)\n",
    "    out_band.FlushCache()\n",
    "\n",
    "    # Define the size of the chunks\n",
    "    chunk_size = 1000  # Adjust based on your system's memory\n",
    "\n",
    "    total_chunks = ((xsize - 1) // chunk_size + 1) * ((ysize - 1) // chunk_size + 1)\n",
    "    processed_chunks = 0\n",
    "    next_progress_threshold = 10\n",
    "\n",
    "    # Process the raster in chunks and write to output file\n",
    "    for i in range(0, xsize, chunk_size):\n",
    "        for j in range(0, ysize, chunk_size):\n",
    "            xoff = i\n",
    "            yoff = j\n",
    "            win_xsize = min(chunk_size, xsize - i)\n",
    "            win_ysize = min(chunk_size, ysize - j)\n",
    "    \n",
    "            elevation = band.ReadAsArray(xoff, yoff, win_xsize, win_ysize)\n",
    "            TRI_chunk = calculate_TRI(elevation, no_data_value)\n",
    "            \n",
    "            processed_chunks += 1\n",
    "            progress = (processed_chunks / total_chunks) * 100\n",
    "            \n",
    "            if progress >= next_progress_threshold:\n",
    "                print(f\"Processing... {progress:.2f}% complete\")\n",
    "                next_progress_threshold += 10\n",
    "    \n",
    "            # Write the TRI_chunk to the output raster\n",
    "            out_band.WriteArray(TRI_chunk, xoff, yoff)\n",
    "    \n",
    "            gc.collect()\n",
    "    # Close the output raster\n",
    "    out_band.FlushCache()\n",
    "    out_raster = None\n",
    "\n",
    "    print(f\"TRI raster saved to {output_tri_file}\")\n",
    "\n",
    "    # Collect garbage after processing each file\n",
    "    gc.collect()\n",
    "\n",
    "print(\"All TRI rasters processed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
